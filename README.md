# Supporting Code for the MSc Thesis Project: Foundational Ontologies to Improve the Explainability of Explainable AI
 
This repository provides the code supporting the master's thesis project presented in [msc_thesis_final_rosazwart.pdf](msc_thesis_final_rosazwart.pdf).

The code performs a drug-repurposing pipeline that predicts new drug candidates that can potentially treat a symptom related to the rare disease Duchenne muscular dystrophy. For the predictions, explanations (in the form of subgraphs of the input graph) are generated. 

In this project, we investigate whether we can improve the explainability of explainable AI (XAI). In order to measure the improvement, we have built upon previous research (https://github.com/PPerdomoQ/rare-disease-explainer) during which a drug-repurposing XAI pipeline has been developed. 

## Building the Knowledge Graphs
A total of two knowledge graphs (KGs) are built in order to compare the achieved explainability given the generated explanations. The first knowledge graph, referred to as the original KG, serves as the baseline and is taken from another project accessed from https://github.com/PPerdomoQ/rare-disease-explainer. Structural changes have been introduced to this knowledge graph resulting in the restructured KG.

### Building the Original Knowledge Graph
A set of Jupyter Notebooks has been taken from https://github.com/PPerdomoQ/rare-disease-explainer used to build the original KG.

- [prev/Notebook_1.ipynb](prev/Notebook_1.ipynb): Builds the knowledge graph with information from Monarch Initiative
- [prev/Notebook_2.ipynb](prev/Notebook_2.ipynb): Fetches information related to drug entities from DrugCentral and Therapeutic Target Database.
- [Notebook_3.ipynb](Notebook_3.ipynb): Merges information from all three data sources

Running this set of Jupyter Notebooks results in the creation of three csv files:

- `output/prev_monarch_associations.csv`
- `output/prev_ttd_associations.csv`
- `output/prev_drugcentral_associations.csv`

Using [main.py](main.py) and specifying in the user input to build the original KG, the graph is built resulting in csv files containing all edges and nodes:

- `output/prev_kg_edges.csv`
- `output/prev_kg_nodes.csv`

### Building the Restructured Knowledge Graph
Running the script [main.py](main.py) and specifying to build the restructured KG, the user is asked whether the data still needs to be fetched from Monarch Initiative. When the user agrees, the necessary information will be fetched from Monarch Initiative as well as from DrugCentral and Therapeutic Target Database:

- `output/monarch_associations.csv`
- `output/ttd_assocations.csv`
- `output/drugcentral_associations.csv`

When the user disagrees, the script will look for the existing `monarch_assocations.csv` file which results in skipping the Monarch Initiative API fetching process. 

After this, the script will merge all data into a KG and restructure the nodes and edges, yielding the restructured KG represented with the following files containing all its edges and nodes:

- `output/kg_edges.csv`
- `output/kg_nodes.csv`

## Generating Predictions
Predictions are generated by training a graph neural network (GNN) model on one of the two KG variations. This process is taken from https://github.com/PPerdomoQ/rare-disease-explainer. However, the script for performing these steps in the pipeline are modified to allow for different input variations while maintaining the essence of the already developed method.

### Node Embedding
For the node embedding step, method Edge2Vec has been implemented in [edge2vec_embedding.ipynb](edge2vec_embedding.ipynb) (use [requirements1.txt](requirements1.txt)). At the start of the Jupyter Notebook, the user can specify which KG is used as input (1: original, 2: restructured). 

The process will create a new folder in which the results from the pipeline will be saved for the given input variation:
- `output/g1_e2v`: Results from performed pipeline using original KG as input
- `output/g2_e2v`: Results from performed pipeline using restructured KG as input

These directories contain folders for each independent run (`run_001`, `run_002`, etc.). 

The process stores the resulting node embedding as well as the final transition matrix into the current folder.

#### Hyperparameters
The values resulting from hyperparameter optimization using Random Search in [hyperparam_opt_e2v.py](https://github.com/rosazwart/XAIFO-ThesisProject/blob/main/hyperparam_opt_e2v.py) are used to perform the node embedding and shown below:

| Parameters          | Original KG | Restructured KG |
| ------------------- | ----------- | --------------- |
| Number of walks     | 2           | 6               |
| Walk length         | 7           | 7               |
| Embedding dimension | 32          | 64              |
| p                   | 0.70        | 0.75            |
| q                   | 1           | 1               |
| epochs              | 10          | 5               |

The probability of returning to the previous node during a walk through the knowledge graph as part of the Edge2Vec method is expressed with parameter p. Parameter q represents the probability that controls whether the random walks comply with a breadth-first or depth-first search. 

### Training GNN Model
For the GNN model training step, the Jupyter Notebook [predictor.ipynb](predictor.ipynb) (use [requirements1.txt](requirements1.txt)) is used. Again, the user needs to choose which input variation is considered complying to the same numbering as in the previous step (1: original, 2: restructured). 

The process will place the result files into the folder of the latest independent run of the relevant KG variation. Results regarding the obtained best trained model are saved in this folder as well as lists of predictions.

#### Hyperparameters
The values resulting from hyperparameter optimization using Random Search in [hyperparam_opt_e2v.py](https://github.com/rosazwart/XAIFO-ThesisProject/blob/main/hyperparam_opt_e2v.py) are used to perform the training process of the GNN model and shown below:

| Parameters           | Original KG | Restructured KG |
| -------------------- | ----------- | --------------- |
| Hidden dimension     | 256         | 256             |
| Output dimension     | 64          | 64              |
| Layers               | 2           | 2               |
| Aggregation function | mean        | mean            |
| Dropout              | 0.2         | 0.2             |
| Learning rate        | 0.07000     | 0.01348         |
| Epochs               | 150         | 100             |

### Hyperparameter Optimization
Hyperparameter optimization of the model using the different input variations has been performed in a separate script file being [hyperparam_opt_e2v.py](hyperparam_opt_e2v.py) (use [requirements1.txt](requirements1.txt)). Again, the user needs to choose which input variation is considered complying to the same numbering as in the previous step (1: original, 2: restructured).

### Comparing Predictions
The accuracy and predictions are compared between the different KG variations used as input of the pipeline. This is performed by the Jupyter Notebook [prediction_analyser.ipynb](prediction_analyser.ipynb) which looks at the output files from the GNN model training step.

This step is important for performing the explanation generation process of the pipeline as it will acquire the list of overlapping predicted drug-symptom pairs. This overlap can be adjusted on percentage over all independent runs to deal with GNN model prediction inconsistency per run.

## Explanations
Explanations are generated by implementing the GNNExplainer algorithm on predictions from one of the two KG variations. This process is taken from https://github.com/PPerdomoQ/rare-disease-explainer. However, the script for performing this step is modified to allow for different input variations while maintaining the essence of the already developed method.

### Generating Explanations
Explanations are generated with [explainer1.ipynb](explainer1.ipynb) (use [requirements1.txt](requirements1.txt)). Again, the user needs to choose which input variation is considered complying to the same numbering as in the previous step (1: original, 2: restructured). 

This step outputs explanations given a list of overlapping drug-symptom pairs throughout all independent runs (yielded by [prediction_analyser.ipynb](prediction_analyser.ipynb)). The explanations are found in the output folder of the relevant input variation in a folder with the prefix `expl_`. The number following up this prefix refers to the minimum number of runs needed for a drug-symptom pair to appear in to be considered as input of the explanation generator.

#### Hyperparameters
The values resulting from hyperparameter optimization using Random Search in [hyperparam_opt_e2v.py](https://github.com/rosazwart/XAIFO-ThesisProject/blob/main/hyperparam_opt_e2v.py) are used to generate explanations with GNNExplainer and shown below:

| Parameters               | Original KG | Restructured KG |
| ------------------------ | ----------- | --------------- |
| Epochs                   | 700         | 700             |
| Number of hops           | 1           | 1               |
| Maximum size explanation | 15          | 15              |
| Search iterations        | 10          | 10              |

### Comparing Explanations
In [explanation_analyser.ipynb](explanation_analyser.ipynb), the explanations on both input variations are compared to each other by looking at objective measurements such as average node types, edge types, triples found for each set of explanations.

In the folder questionnaires, the pdf files show the questionnaires used for obtaining the subjective measurement of explainability. The results are contained in the csv files found in the same directory. 
